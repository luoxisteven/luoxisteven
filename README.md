## Xi Luo (Steven)
- I am a graduate student specializing in Artificial Intelligence, and I am eager to proceed with my career as a Machine Learning Engineer, Software Engineer, Data Analyst, Consultant, and Finance Analyst.
- I have a diverse background, and you may find the answer to why in the section [Personal Philosophy](#personal-philosophy).
- **`If any employer views this profile and shows interest, please don't hesitate to contact me.`**

---
### Contact me
- `Email:` luoxi9932@gmail.com
- `Another Github Account:` [@steveluo32](https://github.com/steveluo32)  
- `Linkedin:` https://www.linkedin.com/in/xi-luo-6259a1208/

### Leanrng Notes
- [Learning Notes](https://github.com/luoxisteven/Learning-Notes)

### About me
- `Domain Knowledge:` Computer Science 🖥️, Machine Learning 🤖, Math 🔢, Statistics 📊, Finance 💰, Economics 📉
- `Program Languages:` Python 🐍, Java 🌋, JavaScript 🕸️, SQL 💽, R 📉, Prolog 😈, Haskell 👻
- `Web Technologies:` React, Vue.js, Spring Boot, Django, Node.js
- `Data Analysis Packages:` NumPy, Pandas, Matplotlib
- `Data Scraping Packages:` Request, Urllib, Selenium, Scrapy
- `Machine Learning Framework:` PyTorch, TensorFlow, Keras, LangChain
- `Hobbies:` All kinds of sports (British Football ⚽️, Gym 🏋️, Skiing 🎿, Bicycle 🚴, 5KM Long Run 🏃), Video Games 🎮 (I’m more of a cloud gamer who prefers watching video game broadcasts while multitasking rather than playing games myself.)

----
### Personal Philosophy

- I enjoy the feeling of earning a living while doing something I love, knowing that it also contributes to my future. I have a strong desire to continuously learn and systemize my knowledge base. 
It's almost like having OCD when it comes to knowledge — I feel a sense of loss if my understanding of something isn't complete or well-organized. My approach to learning is similar to Breadth-First Search (BFS), which I call Breadth-First Learning. I’m not concerned about the depth at first because, over time, BFS will naturally lead to a deeper understanding. 
- In machine learning, nearly 99% of answers are generated based on probability. 
This means that, with a large enough training dataset and a sufficiently complex model (i.e., large enough partitioned space), the model can produce a highly representative answer in most cases. 
My favorite, however, is reinforcement learning, as it genuinely mimics human-like thinking—approaching goals incrementally based on feedback and strategy optimization. This contrasts with other methods where the focus is on minimizing loss (by optimizing a loss function calculated from training data) to fit the most representative answer.
- I really like to map any knowledge into the real world. Computer Science, especially reinforcement learning, is the science of computing the real world, transforming human life from abstract concepts into numbers. It uses numerical calculations to determine how to make optimal decisions.
Our lives are similar to an agent-based scenario, where we have different starting points, measure happiness as the reward function, efforts as the cost function, and use intuition and logic as the heuristics function, continuously adapting through online learning.
How closely we approach optimal happiness depends on how effectively our heuristics (i.e., intuition and logic) guide our actions. 
- We can mimic reinforcement learning algorithms to decide what we should do next by breaking down our final goal into smaller subtasks. Let’s say our ultimate goal is “to live in happiness.” If the situation is complex and hard to navigate, I recommend using a simple approach like Best-First Search. It’s straightforward and doesn’t require much deliberation. On the other hand, if the goal can be clearly divided into manageable and well-organized subtasks, I recommend using more optimal algorithms, such as A*. This involves mapping the subtasks in our mind, considering their associated costs and rewards, and choosing the best option based on this algorithmic approach.
- As we grow older, we develop better heuristics compared to when we were younger, thanks to the accumulation of experience. This enables us to better assess our current status in relation to our goals. However, the most important thing to remember is that no matter how refined our heuristics are or how much experience we have, heuristics remain approximations, and experience is just that—experience. There is no way to estimate the exact cost or reward with complete certainty. What’s even more challenging is that human life can often be modeled as a Partially Observable Markov Decision Process (POMDP). This is because rewards and costs are often probabilistic rather than deterministic. And as we know, solving a POMDP is far more complex than a standard Markov Decision Process (MDP), right? In conclusion, even when we make the right decision based on our heuristics, the outcome often boils down to luck. So, don’t be frustrated if the result doesn’t meet your expectations. Sometimes, it’s just a matter of luck.